{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6857f8c",
   "metadata": {},
   "source": [
    "##### Name: Stuti Upadhyay\n",
    "##### Campus ID: XT81177\n",
    "##### Instructor: Chalachew Jemberie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7599425e",
   "metadata": {},
   "source": [
    "## Homework-Logistic Regression\n",
    "\n",
    "\n",
    "### Dataset\n",
    "\n",
    "In this homework, we will use the California Housing Prices data from [Kaggle](https://www.kaggle.com/datasets/camnugent/california-housing-prices).\n",
    "\n",
    "\n",
    "We'll keep working with the `'median_house_value'` variable, and we'll transform it to a classification task. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9505b471",
   "metadata": {},
   "source": [
    "### Features\n",
    "\n",
    "For the rest of the homework, you'll need to use only these columns:\n",
    "\n",
    "* `'latitude'`,\n",
    "* `'longitude'`,\n",
    "* `'housing_median_age'`,\n",
    "* `'total_rooms'`,\n",
    "* `'total_bedrooms'`,\n",
    "* `'population'`,\n",
    "* `'households'`,\n",
    "* `'median_income'`,\n",
    "* `'median_house_value'`,\n",
    "* `'ocean_proximity'`,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ba603c",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "\n",
    "* Select only the features from above and fill in the missing values with median.\n",
    "* Create a new column `rooms_per_household` by dividing the column `total_rooms` by the column `households` from dataframe. \n",
    "* Create a new column `bedrooms_per_room` by dividing the column `total_bedrooms` by the column `total_rooms` from dataframe. \n",
    "* Create a new column `population_per_household` by dividing the column `population` by the column `households` from dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf581a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, Ridge\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv(\"housing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f721552e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the specified features\n",
    "selected_features = ['latitude', 'longitude', 'housing_median_age', 'total_rooms', \n",
    "                     'total_bedrooms', 'population', 'households', 'median_income', \n",
    "                     'median_house_value', 'ocean_proximity']\n",
    "housing_data = data[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddf65205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values for numerical columns with median\n",
    "numerical_cols = housing_data.select_dtypes(include=['number']).columns\n",
    "housing_data[numerical_cols] = housing_data[numerical_cols].fillna(housing_data[numerical_cols].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73177808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new columns\n",
    "housing_data['rooms_per_household'] = housing_data['total_rooms'] / housing_data['households']\n",
    "housing_data['bedrooms_per_room'] = housing_data['total_bedrooms'] / housing_data['total_rooms']\n",
    "housing_data['population_per_household'] = housing_data['population'] / housing_data['households']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37d8c7f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   latitude  longitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
      "0     37.88    -122.23                41.0        880.0           129.0   \n",
      "1     37.86    -122.22                21.0       7099.0          1106.0   \n",
      "2     37.85    -122.24                52.0       1467.0           190.0   \n",
      "3     37.85    -122.25                52.0       1274.0           235.0   \n",
      "4     37.85    -122.25                52.0       1627.0           280.0   \n",
      "\n",
      "   population  households  median_income  median_house_value ocean_proximity  \\\n",
      "0       322.0       126.0         8.3252            452600.0        NEAR BAY   \n",
      "1      2401.0      1138.0         8.3014            358500.0        NEAR BAY   \n",
      "2       496.0       177.0         7.2574            352100.0        NEAR BAY   \n",
      "3       558.0       219.0         5.6431            341300.0        NEAR BAY   \n",
      "4       565.0       259.0         3.8462            342200.0        NEAR BAY   \n",
      "\n",
      "   rooms_per_household  bedrooms_per_room  population_per_household  \n",
      "0             6.984127           0.146591                  2.555556  \n",
      "1             6.238137           0.155797                  2.109842  \n",
      "2             8.288136           0.129516                  2.802260  \n",
      "3             5.817352           0.184458                  2.547945  \n",
      "4             6.281853           0.172096                  2.181467  \n"
     ]
    }
   ],
   "source": [
    "# Display the updated dataframe\n",
    "print(housing_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816ee6b4",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "What is the most frequent observation (mode) for the column `ocean_proximity`?\n",
    "\n",
    "Options:\n",
    "* `NEAR BAY`\n",
    "* `<1H OCEAN`\n",
    "* `INLAND`\n",
    "* `NEAR OCEAN`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24102a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode of ocean_proximity: <1H OCEAN\n"
     ]
    }
   ],
   "source": [
    "mode_ocean_proximity = housing_data['ocean_proximity'].mode()[0]\n",
    "print(\"Mode of ocean_proximity:\", mode_ocean_proximity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74d40c2",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "* Create the [correlation matrix](https://www.google.com/search?q=correlation+matrix) for the numerical features of your train dataset.\n",
    "    - In a correlation matrix, you compute the correlation coefficient between every pair of features in the dataset.\n",
    "* What are the two features that have the biggest correlation in this dataset?\n",
    "\n",
    "Options:\n",
    "* `total_bedrooms` and `households`\n",
    "* `total_bedrooms` and `total_rooms`\n",
    "* `population` and `households`\n",
    "* `population_per_household` and `total_rooms`\n",
    "\n",
    "\n",
    "### Make `median_house_value` binary\n",
    "\n",
    "* We need to turn the `median_house_value` variable from numeric into binary.\n",
    "* Let's create a variable `above_average` which is `1` if the `median_house_value` is above its mean value and `0` otherwise.\n",
    "\n",
    "### Split the data\n",
    "\n",
    "* Split your data in train/val/test sets, with 60%/20%/20% distribution.\n",
    "* Use Scikit-Learn for that (the `train_test_split` function) and set the seed to 42.\n",
    "* Make sure that the target value (`median_house_value`) is not in your dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7a0bb63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          latitude  longitude  housing_median_age  \\\n",
      "latitude                  1.000000  -0.924664            0.011173   \n",
      "longitude                -0.924664   1.000000           -0.108197   \n",
      "housing_median_age        0.011173  -0.108197            1.000000   \n",
      "total_rooms              -0.036100   0.044568           -0.361262   \n",
      "total_bedrooms           -0.066484   0.069120           -0.319026   \n",
      "population               -0.108785   0.099773           -0.296244   \n",
      "households               -0.071035   0.055310           -0.302916   \n",
      "median_income            -0.079809  -0.015176           -0.119034   \n",
      "median_house_value       -0.144160  -0.045967            0.105623   \n",
      "rooms_per_household       0.106389  -0.027540           -0.153277   \n",
      "bedrooms_per_room        -0.098619   0.081205            0.135622   \n",
      "population_per_household  0.002366   0.002476            0.013191   \n",
      "\n",
      "                          total_rooms  total_bedrooms  population  households  \\\n",
      "latitude                    -0.036100       -0.066484   -0.108785   -0.071035   \n",
      "longitude                    0.044568        0.069120    0.099773    0.055310   \n",
      "housing_median_age          -0.361262       -0.319026   -0.296244   -0.302916   \n",
      "total_rooms                  1.000000        0.927058    0.857126    0.918484   \n",
      "total_bedrooms               0.927058        1.000000    0.873535    0.974366   \n",
      "population                   0.857126        0.873535    1.000000    0.907222   \n",
      "households                   0.918484        0.974366    0.907222    1.000000   \n",
      "median_income                0.198050       -0.007617    0.004834    0.013033   \n",
      "median_house_value           0.134153        0.049457   -0.024650    0.065843   \n",
      "rooms_per_household          0.133798        0.001765   -0.072213   -0.080598   \n",
      "bedrooms_per_room           -0.187381        0.071649    0.010035    0.034498   \n",
      "population_per_household    -0.024581       -0.028325    0.069863   -0.027309   \n",
      "\n",
      "                          median_income  median_house_value  \\\n",
      "latitude                      -0.079809           -0.144160   \n",
      "longitude                     -0.015176           -0.045967   \n",
      "housing_median_age            -0.119034            0.105623   \n",
      "total_rooms                    0.198050            0.134153   \n",
      "total_bedrooms                -0.007617            0.049457   \n",
      "population                     0.004834           -0.024650   \n",
      "households                     0.013033            0.065843   \n",
      "median_income                  1.000000            0.688075   \n",
      "median_house_value             0.688075            1.000000   \n",
      "rooms_per_household            0.326895            0.151948   \n",
      "bedrooms_per_room             -0.545298           -0.233303   \n",
      "population_per_household       0.018766           -0.023737   \n",
      "\n",
      "                          rooms_per_household  bedrooms_per_room  \\\n",
      "latitude                             0.106389          -0.098619   \n",
      "longitude                           -0.027540           0.081205   \n",
      "housing_median_age                  -0.153277           0.135622   \n",
      "total_rooms                          0.133798          -0.187381   \n",
      "total_bedrooms                       0.001765           0.071649   \n",
      "population                          -0.072213           0.010035   \n",
      "households                          -0.080598           0.034498   \n",
      "median_income                        0.326895          -0.545298   \n",
      "median_house_value                   0.151948          -0.233303   \n",
      "rooms_per_household                  1.000000          -0.370308   \n",
      "bedrooms_per_room                   -0.370308           1.000000   \n",
      "population_per_household            -0.004852           0.002601   \n",
      "\n",
      "                          population_per_household  \n",
      "latitude                                  0.002366  \n",
      "longitude                                 0.002476  \n",
      "housing_median_age                        0.013191  \n",
      "total_rooms                              -0.024581  \n",
      "total_bedrooms                           -0.028325  \n",
      "population                                0.069863  \n",
      "households                               -0.027309  \n",
      "median_income                             0.018766  \n",
      "median_house_value                       -0.023737  \n",
      "rooms_per_household                      -0.004852  \n",
      "bedrooms_per_room                         0.002601  \n",
      "population_per_household                  1.000000  \n"
     ]
    }
   ],
   "source": [
    "# Select only numerical features\n",
    "numerical_columns = housing_data.select_dtypes(include=['number']).columns\n",
    "housing_data_numeric = housing_data[numerical_columns]\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = housing_data_numeric.corr()\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6a41066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X = housing_data.drop(columns=['median_house_value'])  # Features\n",
    "y = (housing_data['median_house_value'] > housing_data['median_house_value'].mean()).astype(int)  # Target variable\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca23453",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "* Calculate the *mutual information score* between `above_average` and `ocean_proximity` . Use the training set only.\n",
    "* Round it to 2 decimals using `round(score, 2)`\n",
    "* What is their *mutual information score*?\n",
    "\n",
    "\n",
    "Options:\n",
    "- 0.26\n",
    "- 0\n",
    "- 0.10\n",
    "- 0.16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9d6a86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# One-hot encode the 'ocean_proximity' column\n",
    "encoder = OneHotEncoder()\n",
    "X_train_encoded = encoder.fit_transform(X_train[['ocean_proximity']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a941a0f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Compute mutual information score\n",
    "mi_score = mutual_info_regression(X_train_encoded, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f0efad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mutual information scores: [0.02, 0.1, 0.0, 0.01, 0.01]\n"
     ]
    }
   ],
   "source": [
    "# Compute mutual information score\n",
    "mi_scores_rounded = [round(score, 2) for score in mi_score]\n",
    "print(\"Mutual information scores:\", mi_scores_rounded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a55fa1c",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "* Now let's train a logistic regression\n",
    "* Remember that we have one categorical variable `ocean_proximity` in the data. Include it using one-hot encoding.\n",
    "* Fit the model on the training dataset.\n",
    "    - To make sure the results are reproducible across different versions of Scikit-Learn, fit the model with these parameters:\n",
    "    - `model = LogisticRegression(solver=\"liblinear\", C=1.0, max_iter=1000, random_state=42)`\n",
    "* Calculate the accuracy on the validation dataset and round it to 2 decimal digits.\n",
    "\n",
    "Options:\n",
    "- 0.60\n",
    "- 0.72\n",
    "- 0.84\n",
    "- 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a264ee11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode categorical variable\n",
    "X_train_encoded = pd.get_dummies(X_train, columns=['ocean_proximity'])\n",
    "X_val_encoded = pd.get_dummies(X_val, columns=['ocean_proximity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ac03f26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000, random_state=42, solver=&#x27;liblinear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000, random_state=42, solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=1000, random_state=42, solver='liblinear')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train logistic regression model\n",
    "model = LogisticRegression(solver=\"liblinear\", C=1.0, max_iter=1000, random_state=42)\n",
    "model.fit(X_train_encoded, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d11ef56",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on validation set: 0.83\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy on validation set\n",
    "val_predictions = model.predict(X_val_encoded)\n",
    "accuracy = accuracy_score(y_val, val_predictions)\n",
    "rounded_accuracy = round(accuracy, 2)\n",
    "print(\"Accuracy on validation set:\", rounded_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b55c52e",
   "metadata": {},
   "source": [
    "### Question 5 \n",
    "\n",
    "* Let's find the least useful feature using the *feature elimination* technique.\n",
    "* Train a model with all these features (using the same parameters as in Q4).\n",
    "* Now exclude each feature from this set and train a model without it. Record the accuracy for each model.\n",
    "* For each feature, calculate the difference between the original accuracy and the accuracy without the feature. \n",
    "* Which of following feature has the smallest difference? \n",
    "   * `total_rooms`\n",
    "   * `total_bedrooms` \n",
    "   * `population`\n",
    "   * `households`\n",
    "\n",
    "> **Note**: the difference doesn't have to be positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fdd7dc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train logistic regression model with all features\n",
    "model_all_features = LogisticRegression(solver=\"liblinear\", C=1.0, max_iter=1000, random_state=42)\n",
    "model_all_features.fit(X_train_encoded, y_train)\n",
    "accuracy_all_features = accuracy_score(y_val, model_all_features.predict(X_val_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5761221b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionary to store feature importance differences\n",
    "feature_importance_diff = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "028cfa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude each feature and train a model without it\n",
    "for feature in X_train_encoded.columns:\n",
    "    X_train_subset = X_train_encoded.drop(columns=[feature])\n",
    "    X_val_subset = X_val_encoded.drop(columns=[feature])\n",
    "    \n",
    "    model_subset = LogisticRegression(solver=\"liblinear\", C=1.0, max_iter=1000, random_state=42)\n",
    "    model_subset.fit(X_train_subset, y_train)\n",
    "    \n",
    "    accuracy_subset = accuracy_score(y_val, model_subset.predict(X_val_subset))\n",
    "    \n",
    "    # Calculate difference in accuracy\n",
    "    diff = accuracy_all_features - accuracy_subset\n",
    "    feature_importance_diff[feature] = diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8fb7cf88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature with smallest difference: population_per_household\n"
     ]
    }
   ],
   "source": [
    "# Find the feature with the smallest difference\n",
    "smallest_diff_feature = min(feature_importance_diff, key=feature_importance_diff.get)\n",
    "print(\"Feature with smallest difference:\", smallest_diff_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e702b8",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "* For this question, we'll see how to use a linear regression model from Scikit-Learn\n",
    "* We'll need to use the original column `'median_house_value'`. Apply the logarithmic transformation to this column.\n",
    "* Fit the Ridge regression model (`model = Ridge(alpha=a, solver=\"sag\", random_state=42)`) on the training data.\n",
    "* This model has a parameter `alpha`. Let's try the following values: `[0, 0.01, 0.1, 1, 10]`\n",
    "* Which of these alphas leads to the best RMSE on the validation set? Round your RMSE scores to 3 decimal digits.\n",
    "\n",
    "If there are multiple options, select the smallest `alpha`.\n",
    "\n",
    "Options:\n",
    "- 0\n",
    "- 0.01\n",
    "- 0.1\n",
    "- 1\n",
    "- 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "253dfc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode categorical variable\n",
    "X_train_encoded = pd.get_dummies(X_train, columns=['ocean_proximity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1da5fb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_encoded = pd.get_dummies(X_val, columns=['ocean_proximity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fedc13e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply logarithmic transformation to 'median_house_value'\n",
    "housing_data['median_house_value_log'] = housing_data['median_house_value'].apply(lambda x: np.log(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b1e9c33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X = housing_data.drop(columns=['median_house_value', 'median_house_value_log'])  # Features\n",
    "y = housing_data['median_house_value_log']  # Target variable\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e6be78eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stuti_up_02/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/stuti_up_02/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/stuti_up_02/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/stuti_up_02/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/stuti_up_02/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Train Ridge regression models with different alpha values\n",
    "alphas = [0, 0.01, 0.1, 1, 10]\n",
    "best_alpha = None\n",
    "best_rmse = float('inf')\n",
    "\n",
    "for alpha in alphas:\n",
    "    model = Ridge(alpha=alpha, solver=\"sag\", random_state=42)\n",
    "    model.fit(X_train_encoded, y_train)\n",
    "    val_predictions = model.predict(X_val_encoded)\n",
    "    rmse = mean_squared_error(y_val, val_predictions, squared=False)\n",
    "    \n",
    "    if rmse < best_rmse:\n",
    "        best_rmse = rmse\n",
    "        best_alpha = alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4689c067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best RMSE on validation set: 0.531 for alpha = 0\n"
     ]
    }
   ],
   "source": [
    "rounded_rmse = round(best_rmse, 3)\n",
    "print(\"Best RMSE on validation set:\", rounded_rmse, \"for alpha =\", best_alpha)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
